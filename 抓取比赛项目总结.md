## 清华西门子抓取比赛项目总结

### 目的

比赛要求综合运用机器人运动控制、图
像视觉及机器学习等知识，解决科学技术难题和制造业中的工程问题。  解决机器人智能抓取和操作  ，抓取平台如下，

<img src="抓取比赛项目总结.assets/image-20200907084351567.png" alt="image-20200907084351567" style="zoom:67%;" />

### 比赛的任务描述

环境基于 ROS 系统开发。 整个抓取是基于动态抓取  

**包括两个部分**

- 仿真抓取： 基于 Gazebo 物理引擎，从 10 个物理里面，指定抓取 5 个物体，
  每个物体放到对应指定的框格区域  
- 实物抓取： 从 10 个堆叠的物理里面，指定抓取 5 个物体，每个物体对应放
  到指定的框格区域。  

### **实现方案**

由于没有该机械臂进行提前测试，只能在仿真环境下进行对抓取算法进行测试，所以在前期准备的时候将仿真抓取和实物抓取采取完全相同的方法，在仿真环境下算法能够表现良好后在应用于实际机械臂上。搭建的仿真环境如下

<img src="抓取比赛项目总结.assets/image-20200907090456427.png" alt="image-20200907090456427" style="zoom: 50%;" />

### **算法流程**

设计算法将整个流程分为以下几个各部分，视觉标定，物体检测，抓取位姿计算，机器人抓取。

#### **视觉标定**

视觉标定主要分为kinect相机的标定和机器人手眼标定。Kinect相机可以通过其官方标定程序进行相机标定和深度图配准等。

手眼标定：基于opencv的手眼标定函数，其具体过程如下：

对于eye-to-hand情况，相机相对于机器人的基坐标是固定值，标定时将标定板固定在机器人的末端，移动机械臂，使相机能够拍摄到各个 各个位姿下棋盘的标定板，记录次拍摄时机器人的位姿。然后就是求解$^{base}T_{camera}$

罗列处所有变量：

- 机器人末端相对于机器人base的变换矩阵是可以获得的，即$^{end}T_{base}$已知
- 相机标定过程中可以得到标定keyi板相对于相机的旋转矩阵

由于棋盘和机器人末端之间是固定的，所以$^{board}T_{end}$，是固定不变的，可以根据测量得到。

根据已知条件和固定可求的变量，可以得到如下等式

$$
{}^{end}T_{baseA} {}^{baseA}T_{cameraA} {}^{cameraA}T_{board} =  {}^{end}T_{baseB} {}^{baseB}T_{cameraB} {}^{cameraB}T_{board}
$$
具体

#### **YOLO目标检测**

该部分利用现有的目标检测算法，采用yolov3目标检测算法，对比赛给出了物体进行训练，获取到较好的模型后，实际运用时调用模型进行识别。

**yolo的核心思想**

- YOLO 的核心思想就是利用整张图作为网络的输入，直接在输出层回归 bounding box（边界框） 的位置及其所属的类别
- faster-RCNN 中也直接用整张图作为输入，但是 faster-RCNN 整体还是采用了RCNN 那种 proposal+classifier 的思想，只不过是将提取 proposal 的步骤放在 CNN 中实现了，而 YOLO 则采用直接回归的思路。



#### **抓取位姿计算**

对目标物体进行分析，目标物体一般处于以下几种形状，即球形、柱形等比较规则的物体。所以可以通过寻找物体的主方向，，然后通过坐标变换以及机器人手眼标定，求出物体主方向在机器人坐标系下的位姿。

